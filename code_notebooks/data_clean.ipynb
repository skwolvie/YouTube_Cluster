{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/sk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/sk/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from ast import literal_eval\n",
    "from sklearn import preprocessing\n",
    "from preprocessor import api as p\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "punc = string.punctuation\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "\n",
    "def removeNumbers(text):\n",
    "    \"\"\" Removes integers \"\"\"\n",
    "    text = ''.join([i for i in text if not i.isdigit()])         \n",
    "    return text\n",
    "\n",
    "def remove_punc(text):\n",
    "    \"\"\"custom function to remove the frequent words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in punc])\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(w) for w in word_tokenize(text)])\n",
    "\n",
    "def stem_text(text):\n",
    "    return \" \".join([stemmer.stem(w) for w in word_tokenize(text)])\n",
    "\n",
    "def f(x):\n",
    "    try:\n",
    "        return literal_eval(str(x))   \n",
    "    except Exception as e:\n",
    "        #print(x, e)\n",
    "        return []\n",
    "    \n",
    "def remove_spaces(text):\n",
    "    return \" \".join([word for word in str(text).split()])\n",
    "\n",
    "def listit(x):\n",
    "    try: \n",
    "        return x.split(',')\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "def delinklistit(x):\n",
    "    try: \n",
    "        op=[]\n",
    "        a= x.split(',')\n",
    "        for i in a:\n",
    "            op.append(i.split('/')[-1])\n",
    "        return op\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "def dedup(l):\n",
    "    ul=[]\n",
    "    try:\n",
    "        for i in l:\n",
    "            if i not in ul:\n",
    "                ul.append(i)\n",
    "        return ul\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def stringit(x):\n",
    "    st=\"\"\n",
    "    try:\n",
    "        for i in x:\n",
    "            st= st+', '+i\n",
    "    except:\n",
    "        pass\n",
    "    return st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q4/8qvknrhs5d9dq4q2zxqgqzxr0000gn/T/ipykernel_46990/2773655283.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[\"video_title\"] = df['video_title'].str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv('../data/video_metadata.csv')\n",
    "df['video_tags']= df['video_tags'].apply(lambda x: listit(x))\n",
    "df['video_topics']=df['video_topics'].apply(lambda x: delinklistit(x))\n",
    "df['video_title']= df['video_title'].astype('str')\n",
    "df['video_title']= df['video_title'].str.lower()\n",
    "df['video_title']= df['video_title'].str.strip()\n",
    "df['video_title'] = df['video_title'].apply(lambda text: removeNumbers(text))\n",
    "df['video_title'] = df['video_title'].apply(lambda text: remove_punc(text))\n",
    "df[\"video_title\"] = df['video_title'].str.replace('[^\\w\\s]','')\n",
    "df['video_title'] = df['video_title'].apply(lambda text: remove_stop_words(text))\n",
    "df['video_title'] = df['video_title'].str.strip()\n",
    "df['video_title'] = df['video_title'].apply(lambda text: remove_spaces(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= df.groupby('channel_id')['video_title'].agg(lambda x: \" \".join(x)).reset_index()\n",
    "b= df.groupby('channel_id')[['video_tags', 'video_topics']].agg('sum').reset_index()\n",
    "df= pd.merge(a, b)\n",
    "df['video_topics']= df['video_topics'].apply(lambda x: dedup(x))\n",
    "df['video_tags']= df['video_tags'].apply(lambda x: stringit(x))\n",
    "df['video_topics']= df['video_topics'].apply(lambda x: stringit(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q4/8qvknrhs5d9dq4q2zxqgqzxr0000gn/T/ipykernel_46990/103246213.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[\"channel_descr\"] = df['channel_descr'].str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "df1= pd.read_csv('../data/subscribed_channels.csv')\n",
    "df= pd.merge(df, df1)\n",
    "df= df[['channel_id', 'channel_title', 'channel_descr', 'video_title', 'video_tags', 'video_topics']]\n",
    "df['channel_descr']= df['channel_descr'].str.lower()\n",
    "df['channel_descr']= df['channel_descr'].astype('str')\n",
    "df['channel_descr']= df['channel_descr'].str.strip()\n",
    "df['channel_descr']= df['channel_descr'].str.strip()\n",
    "df['channel_descr'] = df['channel_descr'].apply(lambda text: removeNumbers(text))\n",
    "df['channel_descr'] = df['channel_descr'].apply(lambda text: remove_punc(text))\n",
    "df[\"channel_descr\"] = df['channel_descr'].str.replace('[^\\w\\s]','')\n",
    "df['channel_descr'] = df['channel_descr'].apply(lambda text: remove_stop_words(text))\n",
    "df['channel_descr'] = df['channel_descr'].str.strip()\n",
    "df['channel_descr'] = df['channel_descr'].apply(lambda text: remove_spaces(text))\n",
    "df['corpus_text']= df['channel_descr']+' '+ df['video_title']+ ' '+df['video_tags']+ ' '+df['video_topics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['channel_id', 'channel_title', 'video_topics', 'corpus_text']].to_csv('final_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "169b05a80b39c6e3fc108e186a9aefc48bf46eb305dec0f1d7bd62f7d772e1fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
